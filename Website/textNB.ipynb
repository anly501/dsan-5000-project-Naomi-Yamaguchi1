{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Feature Selection for Text Data\"\n",
    "format:\n",
    "  html:\n",
    "    page-layout: full\n",
    "    code-fold: show\n",
    "    code-copy: true\n",
    "    code-tools: true\n",
    "    code-overflow: wrap\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this segment, we will implement the Naive Bayes technique, specifically tailored for text data. The dataset at hand is sourced from NewsAPI and revolves around various topics, with an additional 'category' column incorporated to facilitate our analysis. Our focus will be narrowed down to the initial 10 documents, aiming to evaluate the model's accuracy and its effectiveness in accurately classifying certain articles.\n",
    "\n",
    "To commence, we will embark on a comprehensive data cleaning process, ensuring that our text data is free from any inconsistencies or unwanted characters that could potentially skew our results. Following this, we will proceed to divide our dataset into training and test sets. This critical step allows us to train our model on one subset of the data and validate its performance on another, ensuring that our evaluations are robust and reliable.\n",
    "\n",
    "Through this meticulous approach, we aim to enhance the performance of our Naive Bayes model, providing us with insightful and accurate classifications of the news articles based on their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import sklearn \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a round-up of the talks from gaconf usa 2023</td>\n",
       "      <td>gaconf returned to the us this week with a ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>american can prevent (and control) type 2 diab...</td>\n",
       "      <td>usa today's health team spoke with scores of e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the steep cost of type 2: when diabetes dragge...</td>\n",
       "      <td>the nation's disjointed and confusing health c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a hidden system of exploitation underpins us h...</td>\n",
       "      <td>this series was produced in partnership with t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the childcare cliff: $122 billion dollar crisi...</td>\n",
       "      <td>the childcare crisis in america; the harsh tru...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0       a round-up of the talks from gaconf usa 2023   \n",
       "1  american can prevent (and control) type 2 diab...   \n",
       "2  the steep cost of type 2: when diabetes dragge...   \n",
       "3  a hidden system of exploitation underpins us h...   \n",
       "4  the childcare cliff: $122 billion dollar crisi...   \n",
       "\n",
       "                                         description  \n",
       "0  gaconf returned to the us this week with a ser...  \n",
       "1  usa today's health team spoke with scores of e...  \n",
       "2  the nation's disjointed and confusing health c...  \n",
       "3  this series was produced in partnership with t...  \n",
       "4  the childcare crisis in america; the harsh tru...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = pd.read_csv(\"cleaned_data/medicine-cleaned.csv\")\n",
    "\n",
    "text_df.head() #we are only focusing on the first 10 rows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to eliminate to first 10 rows of dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a round-up of the talks from gaconf usa 2023</td>\n",
       "      <td>gaconf returned to the us this week with a ser...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>american can prevent (and control) type 2 diab...</td>\n",
       "      <td>usa today's health team spoke with scores of e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the steep cost of type 2: when diabetes dragge...</td>\n",
       "      <td>the nation's disjointed and confusing health c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a hidden system of exploitation underpins us h...</td>\n",
       "      <td>this series was produced in partnership with t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the childcare cliff: $122 billion dollar crisi...</td>\n",
       "      <td>the childcare crisis in america; the harsh tru...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0       a round-up of the talks from gaconf usa 2023   \n",
       "1  american can prevent (and control) type 2 diab...   \n",
       "2  the steep cost of type 2: when diabetes dragge...   \n",
       "3  a hidden system of exploitation underpins us h...   \n",
       "4  the childcare cliff: $122 billion dollar crisi...   \n",
       "\n",
       "                                         description  category  \n",
       "0  gaconf returned to the us this week with a ser...         0  \n",
       "1  usa today's health team spoke with scores of e...         1  \n",
       "2  the nation's disjointed and confusing health c...         1  \n",
       "3  this series was produced in partnership with t...         2  \n",
       "4  the childcare crisis in america; the harsh tru...         2  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = text_df.iloc[:10]\n",
    "len(text_df)\n",
    "\n",
    "text_df['category'] = [0, 1, 1, 2, 2, 3, 3, 4, 4, 5] \n",
    "text_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are transitioning into a crucial phase of our analysis: partitioning our dataset into training and test subsets. This is a pivotal step when preparing to apply the Naive Bayes algorithm specifically to text data.\n",
    "\n",
    "The rationale behind this split is to ensure that we have a separate set of data on which to train our model (the training set), and another distinct set to evaluate its performance (the test set). By adopting this approach, we can accurately assess how well our model generalizes to unseen data, ensuring the robustness and reliability of our results.\n",
    "\n",
    "Following this data split, we will be well-prepared to implement the Naive Bayes algorithm on our text data, striving for a model that delivers accurate and insightful predictions, thereby enhancing our understanding of the dataset at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(text_df[['title', 'description']], text_df['category'], test_size=0.2, random_state=42)\n",
    "\n",
    "X_train['text'] = X_train['title'] + ' ' + X_train['description']\n",
    "X_test['text'] = X_test['title'] + ' ' + X_test['description']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "Leveraging text data necessitates its transformation into a numerical format, a process known as Text Vectorization, which is crucial for compatibility with machine learning algorithms. Text Vectorization intricately converts textual content into numerical values, creating a structured and analyzable representation of the dataset. This transformation not only facilitates the application of algorithms like Naive Bayes but also enhances the model’s capability to discern patterns and make predictions based on textual inputs. By adopting text vectorization, we bridge the gap between the rich, unstructured data present in text form and the numerical prerequisites of machine learning models, setting the stage for a more accurate and insightful analytical experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_train_vect = vectorizer.fit_transform(X_train['text'])\n",
    "X_test_vect = vectorizer.transform(X_test['text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Upon the successful vectorization of our text data, transforming our textual content into numerical format suitable for machine learning, we seamlessly transition into the Model Training phase. In this critical stage, we employ a Naive Bayes classifier, meticulously trained using our meticulously prepared text data. This probabilistic model, renowned for its efficacy in handling text classification tasks, learns the underlying patterns and associations between the words and their respective categories from the training dataset. By so doing, it equips itself to make informed and accurate predictions on unseen data, showcasing its potential to serve as a robust tool in text-based classification endeavors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify = MultinomialNB()\n",
    "classify.fit(X_train_vect, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "With our data now meticulously partitioned into training and testing sets, we are poised to apply the MultinomialNB() classifier, a variant of Naive Bayes specifically tailored for multinomially distributed data, which is often the case with text. By utilizing this classifier, we aim to make predictions on our test data. Lastly, we can simply calculateaccuracy and print classification report using prediction value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = classify.predict(X_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         1\n",
      "           2       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.33      0.33      0.33         2\n",
      "weighted avg       0.50      0.50      0.50         2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naomiyamaguchi/Library/r-miniconda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/naomiyamaguchi/Library/r-miniconda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/naomiyamaguchi/Library/r-miniconda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/naomiyamaguchi/Library/r-miniconda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/naomiyamaguchi/Library/r-miniconda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/naomiyamaguchi/Library/r-miniconda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', accuracy_score(y_test, y_predict))\n",
    "\n",
    "print('Classification Report:\\n', classification_report(y_test, y_predict))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In conclusion, applying the Naive Bayes classifier to text data in Python encompasses critical stages such as data preprocessing, which entails refining the text for the algorithms; text vectorization, which transforms the text into a machine-readable numerical format; and model training, where the classifier is taught using the processed data. Following these steps is the evaluation of the model’s performance, which might reveal a lower-than-anticipated accuracy. However, this accuracy can potentially be improved by adjusting categories and experimenting with various text datasets, indicating that the model's effectiveness is contingent upon the nature and quality of the data it is fed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
